{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Home Value using IBM Watson Machine Learning and Db2 on Cloud\n",
    "\n",
    "In this notebook, we will be showing you how to create a machine learning model to predict home sales. At the same time, we will be use IBM's Db2 on Cloud database to store and recieve data for this exercise. In this notebook, we will be going through the entire data science process of creating a model which includes: Importing Data, Preprocessing Data, Data Exploration, Data Visualization and Data Modeling. We will try to fitting our data with differeent models and see which one gives us the best resuts. At the end of this notebook, we will be deploying our model using Watson Machine Learning so it can used in an application to predict new home sales price. Hope you guys have fun!\n",
    "\n",
    "### Learning Goals\n",
    "\n",
    "The learning goals of this notebook are:\n",
    "\n",
    "    1. Load data and create dataframe by connecting to IBM Db2 on cloud\n",
    "    2. Explore and Visualize Data\n",
    "    2. Create a Sklearn models and determine which is the best model to use \n",
    "    3. Train different models and evaluate which is the best model to use \n",
    "    4. Persist a model in a Watson Machine Learning repository\n",
    "    5. Predict home value using the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade ibmos2spark\n",
    "!pip install --upgrade watson-machine-learning-client\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. IMPORTING DATA\n",
    "\n",
    "The first step as data scienctist is to import our data from our data source. In real a world application, your data is going to be too big to be stored locally on your computer. Due to that, your data will most likely be stored on the cloud or in some other method. For the purposes of this notebook, our data is store on IBM Db2 on Cloud. In order to retrieve data from there, we are going to import IBM's python model `ibmdbpy` that allows Python users to import data from IBM's databases. Our data is stored in schema `SKP44849` and the table name is `HOME_SALES`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to datasource\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of this step, we should have your data imported from your data source and store in memory, so we can use it to create our model. In the next step, we are going to clean our data and make it ready for exploration and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Exploration\n",
    "\n",
    "In this step, we are going to try and explore our data inorder to gain insight. We hope to be able to make some assumptions of our data before we start modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The count, mean, min and max rows are self-explanatory. The std shows the standard deviation, and the 25%, 50% and 75% rows show the corresponding percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum price of the data\n",
    "minimum_price = np.amin(pd_df['SALEPRICE'])\n",
    "\n",
    "# Maximum price of the data\n",
    "maximum_price = np.amax(pd_df['SALEPRICE'])\n",
    "\n",
    "# Mean price of the data\n",
    "mean_price = np.mean(pd_df['SALEPRICE'])\n",
    "\n",
    "# Median price of the data\n",
    "median_price = np.median(pd_df['SALEPRICE'])\n",
    "\n",
    "# Standard deviation of prices of the data\n",
    "std_price = np.std(pd_df['SALEPRICE'])\n",
    "\n",
    "# Show the calculated statistics\n",
    "print(\"Statistics for housing dataset:\\n\")\n",
    "print(\"Minimum price: ${}\".format(minimum_price)) \n",
    "print(\"Maximum price: ${}\".format(maximum_price))\n",
    "print(\"Mean price: ${}\".format(mean_price))\n",
    "print(\"Median price ${}\".format(median_price))\n",
    "print(\"Standard deviation of prices: ${}\".format(std_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Visualization\n",
    "\n",
    "As a data scientist it is important to make assumptions and hypothesiize about our data as we continue to explore our data. Some assumptions that we can make about the data are: \n",
    "\n",
    "1. Homes with more rooms will naturally worth more. Usually homes with more rooms are bigger and can fit more people, so it is reasonable that they cost more money.\n",
    "2. Homes that have recently been built will cost more. Since they are newer and probably have a better design compared to older houses. \n",
    "3. Having a garage will also increase the price of the house and will increase more as the number of cars the garage can hold increases. \n",
    "4. House Style is usually a personal opinion for the buyer, so it shouldn't have that much impact on the cost of the home sale.\n",
    "\n",
    "These are just a few of the assumptions we can make so far from our data. As we move into the visualizing our data, we hope to see patterns that are hard to notice just by looking at the numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd_df.hist(bins=50, figsize=(30,25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the figures are skewed a little, but most of them have a normal distribution. This is normal to have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable we are going to predict is the SALEPRICE. Letâ€™s look at how much each independent variable correlates with this dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = pd_df.corr()\n",
    "corr_matrix[\"SALEPRICE\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SALEPRICE seems to be increasing when the GARAGECARS and the FULLBATH increase. We can also see a negative correlation between SALEPRICE and FOUNDATION and a couple others. And finally, coefficients close to zero indicate that there is no linear correlation. However, we barely see any cofficients close to zeros, this tells us that all the attributes are important to SALEPRICE, which is the attribute we are predicting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modeling\n",
    "\n",
    "Now that we have cleaned and explored our data. We are ready to build our model that will predict the attribute `SALEPRICE`. One of the hardest part in the process is determining which model to use for a particular problem. However, since we are using Python's machine learning library [scikit-learn](https://scikit-learn.org/stable/), we will be able to build and test different models quickly and determine which one is the best to use. We will be building three models:\n",
    "\n",
    "\n",
    "1. [Linear Regression Model](https://towardsdatascience.com/linear-regression-using-python-b136c91bf0a2)\n",
    "2. [Random Forest Model](https://www.distilnetworks.com/glossary/term/random-forest-model/)\n",
    "3. [Gradient Boosting](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)\n",
    "\n",
    "IMPORT NOTE: THE METRIC RESULTS MAY BE A LITTLE DIFFERENT SINCE WE ARE SHUFFLING THE DATA WHEN WE CALL train_test_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a. Splitting Our Data \n",
    "Before we can build our model, we need to split our data into test and train data. We will also be shuffling our data to make sure there isn't any bias when creating the model. Since having any bias in our model will lower the accuracy of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# train, test = train_test_split(pd_df, test_size=0.2)\n",
    "# print(\"Number of training records: \" + str(train.count()))\n",
    "# print(\"Number of testing records : \" + str(test.count()))\n",
    "\n",
    "\n",
    "y = pd_df['SALEPRICE']\n",
    "\n",
    "pd_df = pd_df.drop(['SALEPRICE','ID'],  axis=1)\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "X_2 = pd_df.apply(le.fit_transform)\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "ft = enc.fit(X_2)\n",
    "\n",
    "onehotlabels = enc.transform(X_2).toarray()\n",
    "onehotlabels.shape\n",
    "\n",
    "#print(onehotlabels[:,len(onehotlabels)-1])\n",
    "\n",
    "# print(onehotlabels)\n",
    "\n",
    "x = X_2\n",
    "\n",
    "#x = np.delete(onehotlabels, -1, axis=1)\n",
    "\n",
    "#y = onehotlabels[:,len(onehotlabels)-1]\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split( x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. Developing The Model with Linear Regression \n",
    "\n",
    "Our first model that we are going to build is a linear regression model. This is one of the simplest models to implement and also has a high accuracy as well. We will be importing the `LinearRegression` module from the `sklearn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression(fit_intercept=False)\n",
    "linear_regression_model = regressor.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have built our model, let's use the model to predict the home sales value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the predictions our model gave us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predictions: \\n', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also take a look at the coefficients for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(x_test)\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regressor.coef_)\n",
    "# print('Linear Regression R squared\": %.4f' % regressor.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's calculate root-mean-square error (RMSE) and see if we can gain more information about our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "lin_mse = mean_squared_error(y_pred, y_test)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print('Linear Regression RMSE: %.4f' % lin_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model was able to predict the value of every house in the test set within $43101.9709 of the real price. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "lin_mae = mean_absolute_error(y_pred, y_test)\n",
    "print('Linear Regression MAE: %.4f' % lin_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4c. Developing The Model with Random Forest\n",
    "\n",
    "Now that we tried to fit our dataset to a linear regression model. Let's try a more complex model and see if our accuracy can improve. We will fit our model to the Random Forest Model in this section. We will be importing the `RandomForestRegressor` module from the `sklearn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "forest_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random Forest R squared\": %.4f' % forest_reg.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = forest_reg.predict(x_test)\n",
    "forest_mse = mean_squared_error(y_pred, y_test)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "print('Random Forest RMSE: %.4f' % forest_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the metrics performed, we are getting much better results compared to the Linear Regression model. In our Linear Regression model, the R2 was around 70%, however now it has increase to 82%! This means that the Random Forest model is fitting our data much better compared to the Linear Regression Model. When comparing our RMSE, it has also decreased significantly alluding that there is less variabliity in our model now. \n",
    "\n",
    "Let's try to fit our dataset to one more model and see if we can improve our metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4d. Developing The Model with Gradient Boosting\n",
    "\n",
    "Our last model we are going to fit our data to is Gradient Boosting. We will be importing the `GradientBoostingRegressor` module from the `sklearn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model = ensemble.GradientBoostingRegressor()\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gradient Boosting R squared\": %.4f' % model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "model_mse = mean_squared_error(y_pred, y_test)\n",
    "model_rmse = np.sqrt(model_mse)\n",
    "print('Gradient Boosting RMSE: %.4f' % model_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the metrics are significantly better than the Linear Regression model, however not as good as the Random Forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Deploying Model\n",
    "\n",
    "From our evalution, we can see that our Random Forest Model performed best out of the models we trained. For this reason we are going to use the that model for deploy. Below are the instructions to deploy our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split( pd_df,y)\n",
    "\n",
    "categorical_feature_mask = (pd_df.dtypes==object)\n",
    "numerical_features = ~categorical_feature_mask\n",
    "\n",
    "\n",
    "preprocess = make_column_transformer(\n",
    "    ( make_pipeline(SimpleImputer(), StandardScaler()), numerical_features  ),\n",
    "    ( OneHotEncoder() , categorical_feature_mask,) )\n",
    "\n",
    "\n",
    "model = make_pipeline( preprocess, RandomForestRegressor())\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the credentials that you got from Watson Machine Learning service\n",
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "wml_credentials = {    \n",
    "    \"apikey\": \"<api key>\",\n",
    "  \"instance_id\": \"<instance id>\",\n",
    "  \"url\": \"<URL>\"\n",
    "}\n",
    "client = WatsonMachineLearningAPIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimes_meta = {\n",
    "    client.runtimes.ConfigurationMetaNames.NAME: \"Home_Sale_Model\", \n",
    "    client.runtimes.ConfigurationMetaNames.DESCRIPTION: \"Home Sale Model hype\", \n",
    "    client.runtimes.ConfigurationMetaNames.PLATFORM: { \"name\": \"python\", \"version\": \"3.6\" }, \n",
    "}\n",
    "runtime_details = client.runtimes.store(runtimes_meta)\n",
    "runtime_details\n",
    "runtime_url = client.runtimes.get_url(runtime_details)\n",
    "runtime_uid = client.runtimes.get_uid(runtime_details)\n",
    "print(\"Runtimes URL: \" + runtime_url)\n",
    "print(\"Runtimes UID: \" + runtime_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_props = {client.repository.ModelMetaNames.NAME: \"Home Sale Model hype\",\n",
    "               client.repository.ModelMetaNames.RUNTIME_UID: runtime_uid\n",
    "              }\n",
    "published_model = client.repository.store_model(model=model, meta_props=model_props)\n",
    "import json\n",
    "published_model_uid = client.repository.get_model_uid(published_model)\n",
    "model_details = client.repository.get_details(published_model_uid)\n",
    "print(json.dumps(model_details, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "created_deployment = client.deployments.create(published_model_uid, name=\"Home_Sale_Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.  Predict using the deployed mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the URL that is to be used for prediction. The prediction URL is obtained from the deployment details of the deployment created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_endpoint = client.deployments.get_scoring_url(created_deployment)\n",
    "print(scoring_endpoint)\n",
    "x_train.iloc[0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the payload for prediction. The payload contains the input records for which predictions has to be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "scoring_payload = {'fields': ['LOTAREA', 'BLDGTYPE', 'HOUSESTYLE', 'OVERALLCOND', 'YEARBUILT',\n",
    "       'ROOFSTYLE', 'EXTERCOND', 'FOUNDATION', 'BSMTCOND', 'HEATING',\n",
    "       'HEATINGQC', 'CENTRALAIR', 'ELECTRICAL', 'FULLBATH', 'HALFBATH',\n",
    "       'BEDROOMABVGR', 'KITCHENABVGR', 'KITCHENQUAL', 'TOTRMSABVGRD',\n",
    "       'FIREPLACES', 'FIREPLACEQU', 'GARAGETYPE', 'GARAGEFINISH', 'GARAGECARS',\n",
    "       'GARAGECOND', 'POOLAREA', 'POOLQC', 'FENCE', 'MOSOLD', 'YRSOLD' ], \n",
    "                   'values': [[9000, '1Fam', '2Story', 9, 1920, 'Hip', 'Gd', 'PConc', 'TA',\n",
    "       'GasA', 'Ex', 'Y', 'SBrkr', 1, 0, 3, 1, 'TA', 7, 0, 'NA', 'Detchd',\n",
    "       'Unf', 2, 'TA', 0, 'NA', 'NA', 7, 2009]]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the method to perform online predictions and display the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = client.deployments.score(scoring_endpoint, scoring_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of our modeling step, we built three models: `Linear Regression`, `Random Forest` and `Gradient Boosting`. We noticed that the `Random Forest` model gave us the best results compared to the other two. We could also say that our `Random Forest` model did the best job in describing our dataset compared to the other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you were able to experience the Data Science process through these steps. As well as learn how to use IBM's Db2 on Cloud database as a data source for future machine learning projects and IBM's Watson Studio as a platform to build models. See you guys next time!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
